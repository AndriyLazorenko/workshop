{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Recurrent neural network\n",
    "by <b style=\"font-size:14px\" align=\"left\"> Ievgen Terpil </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    "1. Neural networks for sentiment analysis\n",
    "2. Recurrent neural network\n",
    "    1. LSTM\n",
    "    2. Pre-trained embeddings\n",
    "    3. Bidirectional and stacked LSTM\n",
    "4. Attention mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Neural networks for sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"90%\" src=\"https://github.com/udsclub/whiskey-sentiment-analysis/blob/master/workshop/img/MLP.png?raw=true\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "e = 32\n",
    "h_1 = 128\n",
    "h_2 = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = 20000\n",
    "MAX_SEQUENCE_LENGTH = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.models import model_from_json\n",
    "from keras.layers import Flatten, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mlp = Sequential()\n",
    "mlp.add(Embedding(MAX_NB_WORDS, e, input_length=MAX_SEQUENCE_LENGTH))\n",
    "mlp.add(Flatten())\n",
    "mlp.add(Dense(h_1, activation='relu'))\n",
    "mlp.add(Dense(h_2, activation='relu'))\n",
    "mlp.add(Dense(1, activation='sigmoid'))\n",
    "mlp.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"80%\" src=\"http://adilmoujahid.com/images/activation.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_1 (Embedding)          (None, 30, 32)        640000      embedding_input_1[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 960)           0           embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 128)           123008      flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 128)           16512       dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 1)             129         dense_2[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 779,649\n",
      "Trainable params: 779,649\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mlp.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Swith on full text mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('../data/movie_reviews.csv')\n",
    "test_data = pd.read_csv('../data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(152610, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10660, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tweet_tokenizer = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "negatives = {\n",
    "    \"didn't\": \"didn_`_t\",\n",
    "    \"couldn't\": \"couldn_`_t\",\n",
    "    \"can't\": \"can_`_t\",\n",
    "    \"don't\": \"don_`_t\",\n",
    "    \"wouldn't\": \"wouldn_`_t\",\n",
    "    \"doesn't\": \"doesn_`_t\",\n",
    "    \"wasn't\": \"wasn_`_t\",\n",
    "    \"weren't\": \"weren_`_t\",\n",
    "    \"shouldn't\":\"shouldn_`_t\",\n",
    "    \"isn't\": \"isn_`_t\",\n",
    "    \"aren't\": \"aren_`_t\",\n",
    "}\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace('<br />', ' ')\n",
    "    text = ' '.join(tweet_tokenizer.tokenize(text))\n",
    "    for k, v in negatives.items():\n",
    "        text = text.replace(k, v)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"a sentimental crowd-pleaser , well-directed by le mccarey , this tale about a priest ( bing crosby ) assigned to a problematic parish was so popular that paramount reteamed the same players for the bells of st . mary's .\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(\"\"\"A sentimental crowd-pleaser, well-directed by Le McCarey, this tale about a priest (Bing Crosby) assigned to a problematic parish was so popular that Paramount reteamed the same players for The Bells of St. Mary's.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_data.loc[:, 'prep_text'] = train_data['text'].map(preprocess)\n",
    "test_data.loc[:, 'prep_text'] = test_data['text'].map(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Padding data\n",
    "\n",
    "Keras [Embedding layer](https://keras.io/layers/embeddings/) turn positive integers (indexes) into dense vectors of fixed size. \n",
    "\n",
    "* 1) Firstly convert words to indexes\n",
    "* 2) Then we padding data\n",
    "\n",
    "['not bad', 'movie is bad'] -> [[0, 4, 10], [2, 3, 4]] -> [[0.25, 0.1], [0.6, -0.2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 1. String -> Int vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 132209 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS, filters='\"#$%&()*+-/:;<=>@[\\\\]^{|}~\\t\\n,.')\n",
    "tokenizer.fit_on_texts(train_data['prep_text'])\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_words = min(MAX_NB_WORDS, len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#save tokenizer\n",
    "with open('tokenizer','wb') as ofile:\n",
    "    pickle.dump(tokenizer, ofile)\n",
    "    ofile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sequences_train = tokenizer.texts_to_sequences(train_data['prep_text'])\n",
    "sequences_test = tokenizer.texts_to_sequences(test_data['prep_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[20, 82, 17]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences(['not bad movie'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 2. Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "padded_sequences_train = pad_sequences(sequences_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "padded_sequences_test = pad_sequences(sequences_test, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def padding(text):\n",
    "    return pad_sequences(tokenizer.texts_to_sequences([text]), maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 20, 82, 17]], dtype=int32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padding('not bad movie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x_train = padded_sequences_train\n",
    "x_test = padded_sequences_test\n",
    "y_train = train_data['label']\n",
    "y_test= test_data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (152610, 30)\n",
      "Shape of label tensor: (152610, 2)\n"
     ]
    }
   ],
   "source": [
    "labels = to_categorical(np.asarray(y_train))\n",
    "print('Shape of data tensor:', x_train.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 152610 samples, validate on 10660 samples\n",
      "Epoch 1/3\n",
      "10s - loss: 0.4742 - acc: 0.7654 - val_loss: 0.4395 - val_acc: 0.7891\n",
      "Epoch 2/3\n",
      "10s - loss: 0.3431 - acc: 0.8455 - val_loss: 0.4455 - val_acc: 0.7925\n",
      "Epoch 3/3\n",
      "11s - loss: 0.1881 - acc: 0.9228 - val_loss: 0.5636 - val_acc: 0.7985\n",
      "CPU times: user 41.5 s, sys: 1.85 s, total: 43.4 s\n",
      "Wall time: 33.1 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12d937e80>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "mlp.fit(x_train, y_train, \n",
    "                     nb_epoch=3,\n",
    "                     batch_size=128,\n",
    "                     verbose=2,\n",
    "                     validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(model, text):\n",
    "    return model.predict(padding(preprocess(text)))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94094473"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(mlp, 'awesome film')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.050300647"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(mlp, 'this film is bad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2203566"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(mlp, 'this film is not bad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.53700453"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(mlp, \"i think the movie is good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43518016"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(mlp, \"i don't think the movie is good\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Recurrent neural network\n",
    "\n",
    "<img width=\"90%\" src=\"https://github.com/udsclub/whiskey-sentiment-analysis/blob/master/workshop/img/RNN.png?raw=true\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM\n",
    "[Long Short Term Memory networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) – usually just called “LSTMs” – are a special kind of RNN, capable of learning long-term dependencies. They work tremendously well on a large variety of problems, and are now widely used.\n",
    "\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><b>Classic RNN</b></th>\n",
    "        <th><b>LSTM</b></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-SimpleRNN.png\"></td>\n",
    "        <td><img src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "LSTMs are explicitly designed to avoid the long-term dependency problem. Remembering information for long periods of time is practically their default behavior, not something they struggle to learn!\n",
    "\n",
    "#### LSTM unit\n",
    "<img width=\"60%\" src=\"https://cdn-images-1.medium.com/max/1600/1*laH0_xXEkFE0lKJu54gkFQ.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "lstm = Sequential()\n",
    "lstm.add(Embedding(nb_words, e, input_length=MAX_SEQUENCE_LENGTH))\n",
    "lstm.add(LSTM(128, dropout_U=0.2, dropout_W=0.2))\n",
    "lstm.add(Dropout(0.2))\n",
    "lstm.add(Dense(1, activation='sigmoid'))\n",
    "lstm.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_17 (Embedding)         (None, 30, 32)        640000      embedding_input_12[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "lstm_17 (LSTM)                   (None, 128)           82432       embedding_17[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)             (None, 128)           0           lstm_17[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_18 (Dense)                 (None, 1)             129         dropout_12[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 722,561\n",
      "Trainable params: 722,561\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 152610 samples, validate on 10660 samples\n",
      "Epoch 1/4\n",
      "183s - loss: 0.4854 - acc: 0.7612 - val_loss: 0.4330 - val_acc: 0.7985\n",
      "Epoch 2/4\n",
      "188s - loss: 0.3934 - acc: 0.8196 - val_loss: 0.3991 - val_acc: 0.8179\n",
      "Epoch 3/4\n",
      "188s - loss: 0.3511 - acc: 0.8413 - val_loss: 0.4012 - val_acc: 0.8189\n",
      "Epoch 4/4\n",
      "193s - loss: 0.3177 - acc: 0.8583 - val_loss: 0.4114 - val_acc: 0.8191\n",
      "CPU times: user 13min 51s, sys: 2min 27s, total: 16min 18s\n",
      "Wall time: 12min 42s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x24418c3c8>"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "lstm.fit(x_train, y_train, \n",
    "                     nb_epoch=4,\n",
    "                     batch_size=128,\n",
    "                     verbose=2,\n",
    "                     validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93790126"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(lstm, 'awesome film')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.057851244"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(lstm, 'this film is bad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46220678"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(lstm, 'this film is not bad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86614245"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(lstm, \"i think the movie is good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47535789"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(lstm, \"i don't think the movie is good\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Pre-trained embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can download Google’s pre-trained model [here](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit). It’s **1.5GB**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word2vec_google = KeyedVectors.load_word2vec_format('../data/GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "word2vec_google.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_embedding(word2vec_model, word):\n",
    "    try:\n",
    "        return word2vec_model.word_vec(word)\n",
    "    except KeyError:\n",
    "        return np.zeros(word2vec_model.syn0norm.shape[1])\n",
    "\n",
    "\n",
    "embedding_weights_google = np.zeros((nb_words, word2vec_google.syn0norm.shape[1]))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_weights_google[i] = get_embedding(word2vec_google, word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lstm_word2vec = Sequential()\n",
    "lstm_word2vec.add(Embedding(nb_words, 300,\n",
    "                        mask_zero=True,\n",
    "                        weights=[embedding_weights_google],\n",
    "                        trainable=False))\n",
    "lstm_word2vec.add(LSTM(128, dropout_U=0.2, dropout_W=0.2))\n",
    "lstm_word2vec.add(Dropout(0.2))\n",
    "lstm_word2vec.add(Dense(1, activation='sigmoid'))\n",
    "lstm_word2vec.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_6 (Embedding)          (None, None, 300)     6000000     embedding_input_6[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                    (None, 128)           219648      embedding_6[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)              (None, 128)           0           lstm_4[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "dense_10 (Dense)                 (None, 1)             129         dropout_4[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 6,219,777\n",
      "Trainable params: 219,777\n",
      "Non-trainable params: 6,000,000\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstm_word2vec.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 152610 samples, validate on 10660 samples\n",
      "Epoch 1/20\n",
      "277s - loss: 0.5380 - acc: 0.7234 - val_loss: 0.4980 - val_acc: 0.7534\n",
      "Epoch 2/20\n",
      "307s - loss: 0.4899 - acc: 0.7556 - val_loss: 0.4465 - val_acc: 0.7923\n",
      "Epoch 3/20\n",
      "285s - loss: 0.4663 - acc: 0.7703 - val_loss: 0.4539 - val_acc: 0.7800\n",
      "Epoch 4/20\n",
      "301s - loss: 0.4498 - acc: 0.7829 - val_loss: 0.4128 - val_acc: 0.8089\n",
      "Epoch 5/20\n",
      "281s - loss: 0.4366 - acc: 0.7909 - val_loss: 0.4013 - val_acc: 0.8129\n",
      "Epoch 6/20\n",
      "267s - loss: 0.4248 - acc: 0.7981 - val_loss: 0.3970 - val_acc: 0.8172\n",
      "Epoch 7/20\n",
      "267s - loss: 0.4140 - acc: 0.8044 - val_loss: 0.3958 - val_acc: 0.8153\n",
      "Epoch 8/20\n",
      "266s - loss: 0.4055 - acc: 0.8093 - val_loss: 0.3744 - val_acc: 0.8303\n",
      "Epoch 9/20\n",
      "278s - loss: 0.3970 - acc: 0.8146 - val_loss: 0.3817 - val_acc: 0.8264\n",
      "Epoch 10/20\n",
      "289s - loss: 0.3894 - acc: 0.8189 - val_loss: 0.3784 - val_acc: 0.8277\n",
      "Epoch 11/20\n",
      "284s - loss: 0.3825 - acc: 0.8218 - val_loss: 0.3633 - val_acc: 0.8371\n",
      "Epoch 12/20\n",
      "284s - loss: 0.3770 - acc: 0.8258 - val_loss: 0.3688 - val_acc: 0.8336\n",
      "Epoch 13/20\n",
      "278s - loss: 0.3702 - acc: 0.8286 - val_loss: 0.3572 - val_acc: 0.8400\n",
      "Epoch 14/20\n",
      "288s - loss: 0.3657 - acc: 0.8317 - val_loss: 0.3626 - val_acc: 0.8356\n",
      "Epoch 15/20\n",
      "289s - loss: 0.3601 - acc: 0.8346 - val_loss: 0.3545 - val_acc: 0.8419\n",
      "Epoch 16/20\n",
      "278s - loss: 0.3545 - acc: 0.8380 - val_loss: 0.3650 - val_acc: 0.8331\n",
      "Epoch 17/20\n",
      "276s - loss: 0.3505 - acc: 0.8399 - val_loss: 0.3591 - val_acc: 0.8366\n",
      "Epoch 18/20\n",
      "291s - loss: 0.3462 - acc: 0.8419 - val_loss: 0.3476 - val_acc: 0.8450\n",
      "Epoch 19/20\n",
      "281s - loss: 0.3414 - acc: 0.8452 - val_loss: 0.3544 - val_acc: 0.8424\n",
      "Epoch 20/20\n",
      "280s - loss: 0.3367 - acc: 0.8471 - val_loss: 0.3684 - val_acc: 0.8336\n",
      "CPU times: user 2h 15min 20s, sys: 22min 56s, total: 2h 38min 17s\n",
      "Wall time: 1h 34min 32s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x28d56a828>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "lstm_word2vec.fit(x_train, y_train, \n",
    "                     nb_epoch=20,\n",
    "                     batch_size=128,\n",
    "                     verbose=2,\n",
    "                     validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.017051214"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(lstm_word2vec, 'this film is bad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.54443789"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(lstm_word2vec, 'this film is not bad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89198178"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(lstm_word2vec, \"i think the movie is good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17547244"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(lstm_word2vec, \"i don't think the movie is bad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save model weights\n",
    "lstm.save_weights(\"lstm.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# serialize model to JSON\n",
    "lstm_json = lstm.to_json()\n",
    "with open(\"lstm.json\", \"w\") as json_file:\n",
    "    json_file.write(lstm_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional and stacked LSTM\n",
    "\n",
    "<img width=\"90%\" src=\"https://github.com/udsclub/whiskey-sentiment-analysis/blob/master/workshop/img/BiRNN.png?raw=true\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "blstm = Sequential()\n",
    "blstm.add(Embedding(nb_words, e, input_length=MAX_SEQUENCE_LENGTH))\n",
    "blstm.add(Bidirectional(LSTM(128, dropout_U=0.2, dropout_W=0.2)))\n",
    "blstm.add(Dropout(0.2))\n",
    "blstm.add(Dense(1, activation='sigmoid'))\n",
    "blstm.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_7 (Embedding)          (None, 30, 32)        640000      embedding_input_7[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional)  (None, 256)           164864      embedding_7[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)              (None, 256)           0           bidirectional_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_11 (Dense)                 (None, 1)             257         dropout_5[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 805,121\n",
      "Trainable params: 805,121\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "blstm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img width=\"90%\" src=\"https://github.com/udsclub/whiskey-sentiment-analysis/blob/master/workshop/img/stacked.png?raw=true\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "slstm = Sequential()\n",
    "slstm.add(Embedding(nb_words, e, input_length=MAX_SEQUENCE_LENGTH))\n",
    "slstm.add(LSTM(128, dropout_U=0.2, dropout_W=0.2, return_sequences=True))\n",
    "slstm.add(Dropout(0.2))\n",
    "slstm.add(LSTM(128, dropout_U=0.2, dropout_W=0.2))\n",
    "slstm.add(Dropout(0.2))\n",
    "slstm.add(Dense(1, activation='sigmoid'))\n",
    "slstm.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_10 (Embedding)         (None, 30, 32)        640000      embedding_input_10[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "lstm_10 (LSTM)                   (None, 30, 128)       82432       embedding_10[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)              (None, 30, 128)       0           lstm_10[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lstm_11 (LSTM)                   (None, 128)           131584      dropout_9[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)             (None, 128)           0           lstm_11[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_13 (Dense)                 (None, 1)             129         dropout_10[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 854,145\n",
      "Trainable params: 854,145\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "slstm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention mechanism\n",
    "\n",
    "<img width=\"90%\" src=\"https://github.com/udsclub/whiskey-sentiment-analysis/blob/master/workshop/img/attention.png?raw=true\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K, initializations\n",
    "from keras.layers import Layer\n",
    "class AttentionLayer(Layer):\n",
    "    '''\n",
    "    Attention layer.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, init='glorot_uniform', **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "        self.init = initializations.get(init)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.Uw = self.init((input_shape[-1],))\n",
    "        self.b = self.init((input_shape[1],))\n",
    "        self.trainable_weights = [self.Uw, self.b]\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, input, mask):\n",
    "        return mask\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        multData = K.exp(K.tanh(K.dot(x, self.Uw) + self.b))\n",
    "        if mask is not None:\n",
    "            multData = mask * multData\n",
    "        output = multData / (K.sum(multData, axis=1) + K.epsilon())[:, None]\n",
    "        return K.reshape(output, (output.shape[0], output.shape[1], 1))\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        newShape = list(input_shape)\n",
    "        newShape[-1] = 1\n",
    "        return tuple(newShape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, merge, Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordsInputs = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32', name='words_input')\n",
    "emb = Embedding(nb_words, e, mask_zero=True)(wordsInputs)\n",
    "word_rnn = Bidirectional(LSTM(128, dropout_U=0.2, dropout_W=0.2, return_sequences=True))(emb)\n",
    "attention = AttentionLayer()(word_rnn)\n",
    "doc_emb = merge([word_rnn, attention], mode=lambda x: x[1] * x[0], output_shape=lambda x: x[0])\n",
    "doc_emb = Lambda(lambda x: K.sum(x, axis=1), output_shape=lambda x: (x[0], x[2]))(doc_emb)\n",
    "output = Dense(1, activation=\"sigmoid\")(doc_emb)\n",
    "\n",
    "model = Model(input=[wordsInputs], output=[output])\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "words_input (InputLayer)         (None, 30)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_16 (Embedding)         (None, 30, 32)        640000      words_input[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_5 (Bidirectional)  (None, 30, 256)       164864      embedding_16[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "attentionlayer_4 (AttentionLayer (None, 30, 1)         286         bidirectional_5[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "merge_3 (Merge)                  (None, 30, 256)       0           bidirectional_5[0][0]            \n",
      "                                                                   attentionlayer_4[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)                (None, 256)           0           merge_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_17 (Dense)                 (None, 1)             257         lambda_3[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 805,407\n",
      "Trainable params: 805,407\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
