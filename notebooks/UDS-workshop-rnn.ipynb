{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Recurrent neural network\n",
    "by <b style=\"font-size:14px\" align=\"left\"> Ievgen Terpil </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    "1. Neural networks for sentiment analysis\n",
    "2. Recurrent neural network\n",
    "    1. LSTM\n",
    "    2. Pre-trained embeddings\n",
    "    3. Bidirectional and stacked LSTM\n",
    "4. Attention mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Neural networks for sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider window with fixed size. Discrete words are mapped to vectors of continuous numbers. Than we concat all words vectors in window and fit the received vector for the next layers of the network. The output layer has one neuron and will use a sigmoid activation to output values of 0 and 1 as predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"90%\" src=\"https://github.com/udsclub/whiskey-sentiment-analysis/blob/master/workshop/img/MLP.png?raw=true\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "e = 32\n",
    "h_1 = 128\n",
    "h_2 = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = 20000\n",
    "MAX_SEQUENCE_LENGTH = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%env KERAS_BACKEND=theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.models import model_from_json\n",
    "from keras.layers import Flatten, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mlp = Sequential()\n",
    "mlp.add(Embedding(MAX_NB_WORDS, e, input_length=MAX_SEQUENCE_LENGTH))\n",
    "mlp.add(Flatten())\n",
    "mlp.add(Dense(h_1, activation='relu'))\n",
    "mlp.add(Dense(h_2, activation='relu'))\n",
    "mlp.add(Dense(1, activation='sigmoid'))\n",
    "mlp.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"80%\" src=\"http://adilmoujahid.com/images/activation.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mlp.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Swith on full text mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('../data/movie_reviews.csv')\n",
    "test_data = pd.read_csv('../data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tweet_tokenizer = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "negatives = {\n",
    "    \"didn't\": \"didn_`_t\",\n",
    "    \"couldn't\": \"couldn_`_t\",\n",
    "    \"can't\": \"can_`_t\",\n",
    "    \"don't\": \"don_`_t\",\n",
    "    \"wouldn't\": \"wouldn_`_t\",\n",
    "    \"doesn't\": \"doesn_`_t\",\n",
    "    \"wasn't\": \"wasn_`_t\",\n",
    "    \"weren't\": \"weren_`_t\",\n",
    "    \"shouldn't\":\"shouldn_`_t\",\n",
    "    \"isn't\": \"isn_`_t\",\n",
    "    \"aren't\": \"aren_`_t\",\n",
    "}\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace('<br />', ' ')\n",
    "    text = ' '.join(tweet_tokenizer.tokenize(text))\n",
    "    for k, v in negatives.items():\n",
    "        text = text.replace(k, v)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preprocess(\"\"\"A sentimental crowd-pleaser, well-directed by Le McCarey, this tale about a priest (Bing Crosby) assigned to a problematic parish was so popular that Paramount reteamed the same players for The Bells of St. Mary's.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_data.loc[:, 'prep_text'] = train_data['text'].map(preprocess)\n",
    "test_data.loc[:, 'prep_text'] = test_data['text'].map(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Padding data\n",
    "\n",
    "Keras [Embedding layer](https://keras.io/layers/embeddings/) turn positive integers (indexes) into dense vectors of fixed size. \n",
    "\n",
    "* 1) Firstly convert words to indexes\n",
    "* 2) Then we padding data\n",
    "\n",
    "['not bad', 'movie is bad'] -> [[0, 4, 10], [2, 3, 4]] -> [[0.25, 0.1], [0.6, -0.2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 1. String -> Int vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS, filters='\"#$%&()*+-/:;<=>@[\\\\]^{|}~\\t\\n,.')\n",
    "tokenizer.fit_on_texts(train_data['prep_text'])\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_words = min(MAX_NB_WORDS, len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#save tokenizer\n",
    "with open('tokenizer','wb') as ofile:\n",
    "    pickle.dump(tokenizer, ofile)\n",
    "    ofile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sequences_train = tokenizer.texts_to_sequences(train_data['prep_text'])\n",
    "sequences_test = tokenizer.texts_to_sequences(test_data['prep_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizer.texts_to_sequences(['not bad movie'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 2. Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "padded_sequences_train = pad_sequences(sequences_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "padded_sequences_test = pad_sequences(sequences_test, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def padding(text):\n",
    "    return pad_sequences(tokenizer.texts_to_sequences([text]), maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "padding('not bad movie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x_train = padded_sequences_train\n",
    "x_test = padded_sequences_test\n",
    "y_train = train_data['label']\n",
    "y_test= test_data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "labels = to_categorical(np.asarray(y_train))\n",
    "print('Shape of data tensor:', x_train.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "mlp.fit(x_train, y_train, \n",
    "                     nb_epoch=3,\n",
    "                     batch_size=128,\n",
    "                     verbose=2,\n",
    "                     validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(model, text):\n",
    "    return model.predict(padding(preprocess(text)))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predict(mlp, 'awesome film')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predict(mlp, 'this film is bad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predict(mlp, 'this film is not bad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predict(mlp, \"i think the movie is good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predict(mlp, \"i don't think the movie is good\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Recurrent neural network\n",
    "\n",
    "The idea behind RNNs is to make use of sequential information. In a traditional neural network we assume that all inputs (and outputs) are independent of each other. But for many tasks that’s a very bad idea. RNNs are called recurrent because they perform the same task for every element of a sequence, with the output being depended on the previous computations.\n",
    "\n",
    "Training a RNN is similar to training a traditional Neural Network. We also use the backpropagation algorithm, but with a little twist. Because the parameters are shared by all time steps in the network, the gradient at each output depends not only on the calculations of the current time step, but also the previous time steps.\n",
    "\n",
    "<img width=\"90%\" src=\"https://github.com/udsclub/whiskey-sentiment-analysis/blob/master/workshop/img/RNN.png?raw=true\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM\n",
    "[Long Short Term Memory networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) – usually just called “LSTMs” – are a special kind of RNN, capable of learning long-term dependencies. They work tremendously well on a large variety of problems, and are now widely used.\n",
    "\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><b>Classic RNN</b></th>\n",
    "        <th><b>LSTM</b></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-SimpleRNN.png\"></td>\n",
    "        <td><img src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "LSTMs are explicitly designed to avoid the long-term dependency problem. Remembering information for long periods of time is practically their default behavior, not something they struggle to learn!\n",
    "\n",
    "#### LSTM unit\n",
    "<img width=\"60%\" src=\"https://cdn-images-1.medium.com/max/1600/1*laH0_xXEkFE0lKJu54gkFQ.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "lstm = Sequential()\n",
    "lstm.add(Embedding(nb_words, e, input_length=MAX_SEQUENCE_LENGTH))\n",
    "lstm.add(LSTM(128, dropout_U=0.2, dropout_W=0.2))\n",
    "lstm.add(Dropout(0.2))\n",
    "lstm.add(Dense(1, activation='sigmoid'))\n",
    "lstm.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_17 (Embedding)         (None, 30, 32)        640000      embedding_input_12[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "lstm_17 (LSTM)                   (None, 128)           82432       embedding_17[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)             (None, 128)           0           lstm_17[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_18 (Dense)                 (None, 1)             129         dropout_12[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 722,561\n",
      "Trainable params: 722,561\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 152610 samples, validate on 10660 samples\n",
      "Epoch 1/4\n",
      "183s - loss: 0.4854 - acc: 0.7612 - val_loss: 0.4330 - val_acc: 0.7985\n",
      "Epoch 2/4\n",
      "188s - loss: 0.3934 - acc: 0.8196 - val_loss: 0.3991 - val_acc: 0.8179\n",
      "Epoch 3/4\n",
      "188s - loss: 0.3511 - acc: 0.8413 - val_loss: 0.4012 - val_acc: 0.8189\n",
      "Epoch 4/4\n",
      "193s - loss: 0.3177 - acc: 0.8583 - val_loss: 0.4114 - val_acc: 0.8191\n",
      "CPU times: user 13min 51s, sys: 2min 27s, total: 16min 18s\n",
      "Wall time: 12min 42s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x24418c3c8>"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#RUN AT HOME!\n",
    "%%time\n",
    "lstm.fit(x_train, y_train, \n",
    "                     nb_epoch=10,\n",
    "                     batch_size=128,\n",
    "                     verbose=2,\n",
    "                     validation_data=(x_test, y_test)\n",
    "                     callbacks=[early_stopping]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93790126"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(lstm, 'awesome film')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.057851244"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(lstm, 'this film is bad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46220678"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(lstm, 'this film is not bad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86614245"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(lstm, \"i think the movie is good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47535789"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(lstm, \"i don't think the movie is good\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Pre-trained embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can download Google’s pre-trained model [here](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit). It’s **1.5GB**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#RUN AT HOME!\n",
    "word2vec_google = KeyedVectors.load_word2vec_format('../data/GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "word2vec_google.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_embedding(word2vec_model, word):\n",
    "    try:\n",
    "        return word2vec_model.word_vec(word)\n",
    "    except KeyError:\n",
    "        return np.zeros(word2vec_model.syn0norm.shape[1])\n",
    "\n",
    "\n",
    "embedding_weights_google = np.zeros((nb_words, word2vec_google.syn0norm.shape[1]))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_weights_google[i] = get_embedding(word2vec_google, word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lstm_word2vec = Sequential()\n",
    "lstm_word2vec.add(Embedding(nb_words, 300,\n",
    "                        mask_zero=True,\n",
    "                        weights=[embedding_weights_google],\n",
    "                        trainable=False))\n",
    "lstm_word2vec.add(LSTM(128, dropout_U=0.2, dropout_W=0.2))\n",
    "lstm_word2vec.add(Dropout(0.2))\n",
    "lstm_word2vec.add(Dense(1, activation='sigmoid'))\n",
    "lstm_word2vec.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_6 (Embedding)          (None, None, 300)     6000000     embedding_input_6[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                    (None, 128)           219648      embedding_6[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)              (None, 128)           0           lstm_4[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "dense_10 (Dense)                 (None, 1)             129         dropout_4[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 6,219,777\n",
      "Trainable params: 219,777\n",
      "Non-trainable params: 6,000,000\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstm_word2vec.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 152610 samples, validate on 10660 samples\n",
      "Epoch 1/20\n",
      "277s - loss: 0.5380 - acc: 0.7234 - val_loss: 0.4980 - val_acc: 0.7534\n",
      "Epoch 2/20\n",
      "307s - loss: 0.4899 - acc: 0.7556 - val_loss: 0.4465 - val_acc: 0.7923\n",
      "Epoch 3/20\n",
      "285s - loss: 0.4663 - acc: 0.7703 - val_loss: 0.4539 - val_acc: 0.7800\n",
      "Epoch 4/20\n",
      "301s - loss: 0.4498 - acc: 0.7829 - val_loss: 0.4128 - val_acc: 0.8089\n",
      "Epoch 5/20\n",
      "281s - loss: 0.4366 - acc: 0.7909 - val_loss: 0.4013 - val_acc: 0.8129\n",
      "Epoch 6/20\n",
      "267s - loss: 0.4248 - acc: 0.7981 - val_loss: 0.3970 - val_acc: 0.8172\n",
      "Epoch 7/20\n",
      "267s - loss: 0.4140 - acc: 0.8044 - val_loss: 0.3958 - val_acc: 0.8153\n",
      "Epoch 8/20\n",
      "266s - loss: 0.4055 - acc: 0.8093 - val_loss: 0.3744 - val_acc: 0.8303\n",
      "Epoch 9/20\n",
      "278s - loss: 0.3970 - acc: 0.8146 - val_loss: 0.3817 - val_acc: 0.8264\n",
      "Epoch 10/20\n",
      "289s - loss: 0.3894 - acc: 0.8189 - val_loss: 0.3784 - val_acc: 0.8277\n",
      "Epoch 11/20\n",
      "284s - loss: 0.3825 - acc: 0.8218 - val_loss: 0.3633 - val_acc: 0.8371\n",
      "Epoch 12/20\n",
      "284s - loss: 0.3770 - acc: 0.8258 - val_loss: 0.3688 - val_acc: 0.8336\n",
      "Epoch 13/20\n",
      "278s - loss: 0.3702 - acc: 0.8286 - val_loss: 0.3572 - val_acc: 0.8400\n",
      "Epoch 14/20\n",
      "288s - loss: 0.3657 - acc: 0.8317 - val_loss: 0.3626 - val_acc: 0.8356\n",
      "Epoch 15/20\n",
      "289s - loss: 0.3601 - acc: 0.8346 - val_loss: 0.3545 - val_acc: 0.8419\n",
      "Epoch 16/20\n",
      "278s - loss: 0.3545 - acc: 0.8380 - val_loss: 0.3650 - val_acc: 0.8331\n",
      "Epoch 17/20\n",
      "276s - loss: 0.3505 - acc: 0.8399 - val_loss: 0.3591 - val_acc: 0.8366\n",
      "Epoch 18/20\n",
      "291s - loss: 0.3462 - acc: 0.8419 - val_loss: 0.3476 - val_acc: 0.8450\n",
      "Epoch 19/20\n",
      "281s - loss: 0.3414 - acc: 0.8452 - val_loss: 0.3544 - val_acc: 0.8424\n",
      "Epoch 20/20\n",
      "280s - loss: 0.3367 - acc: 0.8471 - val_loss: 0.3684 - val_acc: 0.8336\n",
      "CPU times: user 2h 15min 20s, sys: 22min 56s, total: 2h 38min 17s\n",
      "Wall time: 1h 34min 32s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x28d56a828>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#RUN AT HOME!\n",
    "%%time\n",
    "lstm_word2vec.fit(x_train, y_train, \n",
    "                     nb_epoch=20,\n",
    "                     batch_size=128,\n",
    "                     verbose=2,\n",
    "                     validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.017051214"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(lstm_word2vec, 'this film is bad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.54443789"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(lstm_word2vec, 'this film is not bad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89198178"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(lstm_word2vec, \"i think the movie is good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17547244"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(lstm_word2vec, \"i don't think the movie is bad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save model weights\n",
    "lstm.save_weights(\"lstm.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# serialize model to JSON\n",
    "lstm_json = lstm.to_json()\n",
    "with open(\"lstm.json\", \"w\") as json_file:\n",
    "    json_file.write(lstm_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional and stacked LSTM\n",
    "\n",
    "<img width=\"90%\" src=\"https://github.com/udsclub/whiskey-sentiment-analysis/blob/master/workshop/img/BiRNN.png?raw=true\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "blstm = Sequential()\n",
    "blstm.add(Embedding(nb_words, e, input_length=MAX_SEQUENCE_LENGTH))\n",
    "blstm.add(Bidirectional(LSTM(128, dropout_U=0.2, dropout_W=0.2)))\n",
    "blstm.add(Dropout(0.2))\n",
    "blstm.add(Dense(1, activation='sigmoid'))\n",
    "blstm.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_7 (Embedding)          (None, 30, 32)        640000      embedding_input_7[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional)  (None, 256)           164864      embedding_7[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)              (None, 256)           0           bidirectional_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_11 (Dense)                 (None, 1)             257         dropout_5[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 805,121\n",
      "Trainable params: 805,121\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "blstm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img width=\"90%\" src=\"https://github.com/udsclub/whiskey-sentiment-analysis/blob/master/workshop/img/stacked.png?raw=true\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "slstm = Sequential()\n",
    "slstm.add(Embedding(nb_words, e, input_length=MAX_SEQUENCE_LENGTH))\n",
    "# ADD LSTM LAYERS HERE\n",
    "slstm.add(Dense(1, activation='sigmoid'))\n",
    "slstm.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "slstm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention mechanism\n",
    "\n",
    "With an attention mechanism we no longer try encode the full sentence(document) into a fixed-length vector. Rather, we allow the classifier to “attend” to different parts of the sentence. Prediction now depends on a weighted combination of all the RNN states, not just the last state.\n",
    "\n",
    "<img width=\"90%\" src=\"https://github.com/udsclub/whiskey-sentiment-analysis/blob/master/workshop/img/attention.png?raw=true\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K, initializations\n",
    "from keras.layers import Layer\n",
    "class AttentionLayer(Layer):\n",
    "    '''\n",
    "    Attention layer.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, init='glorot_uniform', **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "        self.init = initializations.get(init)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.Uw = self.init((input_shape[-1],))\n",
    "        self.b = self.init((input_shape[1],))\n",
    "        self.trainable_weights = [self.Uw, self.b]\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, input, mask):\n",
    "        return mask\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        multData = K.exp(K.tanh(K.dot(x, self.Uw) + self.b))\n",
    "        if mask is not None:\n",
    "            multData = mask * multData\n",
    "        output = multData / (K.sum(multData, axis=1) + K.epsilon())[:, None]\n",
    "        return K.reshape(output, (output.shape[0], output.shape[1], 1))\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        newShape = list(input_shape)\n",
    "        newShape[-1] = 1\n",
    "        return tuple(newShape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, merge, Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordsInputs = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32', name='words_input')\n",
    "emb = Embedding(nb_words, e, mask_zero=True)(wordsInputs)\n",
    "word_rnn = Bidirectional(LSTM(128, dropout_U=0.2, dropout_W=0.2, return_sequences=True))(emb)\n",
    "attention = AttentionLayer()(word_rnn)\n",
    "doc_emb = merge([word_rnn, attention], mode=lambda x: x[1] * x[0], output_shape=lambda x: x[0])\n",
    "doc_emb = Lambda(lambda x: K.sum(x, axis=1), output_shape=lambda x: (x[0], x[2]))(doc_emb)\n",
    "output = Dense(1, activation=\"sigmoid\")(doc_emb)\n",
    "\n",
    "model = Model(input=[wordsInputs], output=[output])\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "words_input (InputLayer)         (None, 30)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_16 (Embedding)         (None, 30, 32)        640000      words_input[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_5 (Bidirectional)  (None, 30, 256)       164864      embedding_16[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "attentionlayer_4 (AttentionLayer (None, 30, 1)         286         bidirectional_5[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "merge_3 (Merge)                  (None, 30, 256)       0           bidirectional_5[0][0]            \n",
      "                                                                   attentionlayer_4[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)                (None, 256)           0           merge_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_17 (Dense)                 (None, 1)             257         lambda_3[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 805,407\n",
      "Trainable params: 805,407\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
