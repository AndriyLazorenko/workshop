{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--<h1 style=\"font-size:40px; font-family:Verdana\" align=\"center\"> UDS-Club Workshop </h1> -->\n",
    "<h2 style=\"font-size:34px; font-family:Verdana\" align=\"center\"> Char-based Recurrent and Convotional Neural Networks for Text Classification </h2>\n",
    "<img src='http://i.piccy.info/i9/666d78be04fbcf04fdb321d5953d1fa5/1492256847/123248/1137898/ua_parrots.jpg'/>\n",
    "<h4 style=\"font-size:18px; font-family:Verdana\" align=\"right\"> by Vitaliy Radchenko <br> <pre>    2017-04-23</pre> </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height: 1px; background-color: #808080\">\n",
    "## Table of Contents\n",
    "\n",
    "<ol>\n",
    "    <li style=\"font-size:20px; font-family:Verdana\">[Embedding approach](#1)</li>\n",
    "    <li style=\"font-size:20px; font-family:Verdana\">[OHE approach](#2)</li>\n",
    "    <li style=\"font-size:20px; font-family:Verdana\">[Different model architectures](#3)</li>\n",
    "</ol>\n",
    "<hr style=\"height: 1px; background-color: #808080\">\n",
    "## 1. Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installation-based (must be already on-board, see your Docker container)\n",
    "<ol>\n",
    "    <li>Python 3.x (or Anaconda3 for Python 3.5, https://www.continuum.io/downloads)</li>\n",
    "    <li>Scikit-learn 0.18.x (pip install scikit-learn==0.18.1, http://scikit-learn.org/)</li>\n",
    "    <li>TextBlob (pip install textblob, https://textblob.readthedocs.io/en/dev/)</li>\n",
    "    <li>NLTK (pip install nltk, http://www.nltk.org/)\n",
    "    <li>Pandas / NumPy / SciPy</li>\n",
    "</ol>\n",
    "\n",
    "#### Knowledge-based\n",
    "<ol>\n",
    "    <li>Basics of numpy</li>\n",
    "    <li>Embeddings</li>\n",
    "    <li>Recurrent Neural Networks</li>\n",
    "    <li>Convolutional Neural Networks</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the first glance char-based models are the same as word-based models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "BASE_DIR = '../'\n",
    "TEXT_DATA_DIR = BASE_DIR + 'data/'\n",
    "TEXT_DATA_FILE = \"movie_reviews.csv\"\n",
    "HEADER = True\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 1024\n",
    "VALIDATION_SPLIT = 0.06552\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "def load_data():\n",
    "    x = []\n",
    "    y = []\n",
    "    with open(os.path.join(TEXT_DATA_DIR, TEXT_DATA_FILE), \"r\") as f:\n",
    "        if HEADER:\n",
    "            _ = next(f)\n",
    "        for line in f:\n",
    "            temp_y, temp_x = line.rstrip(\"\\n\").split(\",\", 1)\n",
    "            x.append(temp_x)\n",
    "            y.append(temp_y)\n",
    "\n",
    "    return x, y\n",
    "\n",
    "#loading data\n",
    "data, labels = load_data()\n",
    "labels = np.asarray(labels, dtype='int8')\n",
    "\n",
    "#split data on train and validation sets\n",
    "data_train, data_val, labels_train, labels_val = \\\n",
    "    train_test_split(data, np.asarray(labels, dtype='int8'),\n",
    "                     test_size=VALIDATION_SPLIT, random_state=RANDOM_SEED, stratify=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step will be to replace letters with its indexes. \n",
    "\n",
    "**Your task** is to create a vocabulary (include all lowercase english letters, digits, punctuation) and replace chars with indexes. Use library `string` and `keras.preprocessing.sequence.pad_sequences`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-ad5c0cafedf3>, line 20)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-ad5c0cafedf3>\"\u001b[0;36m, line \u001b[0;32m20\u001b[0m\n\u001b[0;31m    X_train =\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def create_vocab_set():\n",
    "    \n",
    "    #1. YOUR CODE\n",
    "    return vocab, vocab_size\n",
    "\n",
    "def text2sequence(text, vocab):\n",
    "    temp = []\n",
    "    #2. YOUR CODE\n",
    "    return temp\n",
    "\n",
    "vocab, vocab_size = create_vocab_set()\n",
    "\n",
    "X_train = text2sequence(data_train, vocab)\n",
    "X_val = text2sequence(data_val, vocab)\n",
    "\n",
    "#3. YOUR CODE\n",
    "X_train = \n",
    "X_val = \n",
    "y_train, y_test = to_categorical(labels_train), to_categorical(labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>Click to see answer</summary>\n",
    "  <img alt=\"Smiley face\" align=\"left\" src=\"http://1.1m.yt/_FT_6m0.png\">\n",
    "  <p align='left'>Are you sure you tried to solve it on your own?</p>\n",
    "      <pre>\n",
    "          <code>\n",
    "              1. alphabet = (list(string.ascii_lowercase) + list(string.digits) +\n",
    "                                list(string.punctuation) + [' ', '\\n'])\n",
    "                 vocab_size = len(alphabet)\n",
    "\n",
    "                 vocab = {}\n",
    "                 for ix, t in enumerate(alphabet):\n",
    "                     vocab[t] = ix\n",
    "              2. for review in text:\n",
    "                     temp.append([])\n",
    "                     for i in review:\n",
    "                         char = vocab.get(i,0)\n",
    "                         if char != 0:\n",
    "                             temp[-1].append(char)\n",
    "              3. X_train = pad_sequences(X_train, maxlen=MAX_SEQUENCE_LENGTH, value=-1)\n",
    "                 X_val = pad_sequences(X_val, maxlen=MAX_SEQUENCE_LENGTH, value=-1)\n",
    "          </code>\n",
    "      </pre>\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways of sentences char-representation. First we will overview the same approach as for words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'vocab_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-312d38992f90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# initialize model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEMBEDDING_DIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_SEQUENCE_LENGTH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_filter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mborder_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'valid'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_filter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mborder_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'valid'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vocab_size' is not defined"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D \n",
    "from keras.layers import Embedding \n",
    "from keras.layers import Dropout, GlobalMaxPooling1D\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping\n",
    "\n",
    "EMBEDDING_DIM = 50\n",
    "\n",
    "# initialize model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, trainable=True))\n",
    "model.add(Conv1D(nb_filter=100, filter_length=4, border_mode='valid', activation='relu'))\n",
    "model.add(Conv1D(nb_filter=100, filter_length=4, border_mode='valid', activation='relu'))\n",
    "model.add(Conv1D(nb_filter=100, filter_length=4, border_mode='valid', activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Embeddings**\n",
    "\n",
    "The same as in previous example of word-based convolutional neural networks. We randomly initialise embedding weights and will improve them during model training.\n",
    "\n",
    "**Convolutions**\n",
    "\n",
    "We should experiment with it and try to find the best architecture for our task. \n",
    "\n",
    "**GlobalMaxPooling1D**\n",
    "\n",
    "With GlobalMaxPooling1D we try to select the most important feature in each filter and pass it to the dense layer\n",
    "\n",
    "**Dense**\n",
    "\n",
    "It is fully-connected layers with dropout between them in case of regularization.\n",
    "\n",
    "This architecture gives 78% on validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another approach of char representation is One-Hot-Encoding. For example, our dictionary consists of 3 letters \"a\", \"b\", \"c\". OHE representation of \"abca\" will be $$a – 0 0 \\\\ b – 1 0 \\\\ c – 1 1 \\\\ a – 0 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do these transformations we will use Keras Model Api and some TensorFlow functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# ohe function\n",
    "def ohe(x, sz):\n",
    "    return tf.to_float(tf.one_hot(x, sz, on_value=1, off_value=0, axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_4 (InputLayer)             (None, 1024)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)                (None, 1024, 70)      0           input_4[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_8 (Convolution1D)  (None, 1021, 100)     28100       lambda_3[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_2 (MaxPooling1D)    (None, 204, 100)      0           convolution1d_8[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_9 (Convolution1D)  (None, 201, 100)      40100       maxpooling1d_2[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_10 (Convolution1D) (None, 198, 100)      40100       convolution1d_9[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                    (None, 128)           117248      convolution1d_10[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "dense_5 (Dense)                  (None, 2)             258         lstm_2[0][0]                     \n",
      "====================================================================================================\n",
      "Total params: 225,806\n",
      "Trainable params: 225,806\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Lambda\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.layers import LSTM\n",
    "\n",
    "\n",
    "# define input\n",
    "in_sentence = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int64')\n",
    "# Lambda Layer for ohe transformation\n",
    "embedded = Lambda(ohe, output_shape=lambda x: (x[0], x[1], vocab_size), arguments={\"sz\": vocab_size})(in_sentence)\n",
    "block = embedded\n",
    "# convolutions with MaxPooling\n",
    "for i in range(3):\n",
    "    block = Conv1D(nb_filter=100, filter_length=4, border_mode='valid', activation='relu')(block)\n",
    "    if i == 0:\n",
    "        block = MaxPooling1D(pool_length=5)(block)\n",
    "# LSTM cell\n",
    "block = LSTM(128, dropout_U=0.1, dropout_W=0.1)(block)\n",
    "block = Dense(100, activation='relu')(block)\n",
    "block = Dense(2, activation='sigmoid')(block)\n",
    "# gather model\n",
    "model = Model(input=in_sentence, output=block)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model gives better result – 80% on validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input**\n",
    "\n",
    "Define input shape and its type.\n",
    "\n",
    "**Lambda**\n",
    "\n",
    "Custom Layer for OHE transfromations. Main arguments are function and output shape.\n",
    "\n",
    "**Convolutions with MaxPooling**\n",
    "\n",
    "Convolutions with MaxPooling select several best features and feed them to LSTM.\n",
    "\n",
    "**LSTM**\n",
    "\n",
    "<details>\n",
    "  <summary>Why LSTM should be better than Dense Layer?</summary>\n",
    "  <img alt=\"Smiley face\" align=\"left\" src=\"http://1.1m.yt/_FT_6m0.png\">\n",
    "  <p align='left'>Are you sure you tried to solve it on your own?</p>\n",
    "      <pre>\n",
    "            For Dense Layer we lose information about feautures order. LSTM can understand and learn it.\n",
    "      </pre>\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was two main approaches. Let's try to build new architecture. ![image]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_12 (InputLayer)            (None, 1024)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "lambda_11 (Lambda)               (None, 1024, 70)      0           input_12[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_61 (Convolution1D) (None, 1020, 100)     35100       lambda_11[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_62 (Convolution1D) (None, 1020, 100)     35100       lambda_11[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_63 (Convolution1D) (None, 1020, 100)     35100       lambda_11[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_23 (MaxPooling1D)   (None, 340, 100)      0           convolution1d_61[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_24 (MaxPooling1D)   (None, 340, 100)      0           convolution1d_62[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_25 (MaxPooling1D)   (None, 340, 100)      0           convolution1d_63[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "merge_12 (Merge)                 (None, 1020, 100)     0           maxpooling1d_23[0][0]            \n",
      "                                                                   maxpooling1d_24[0][0]            \n",
      "                                                                   maxpooling1d_25[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_64 (Convolution1D) (None, 1016, 100)     50100       merge_12[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_26 (MaxPooling1D)   (None, 203, 100)      0           convolution1d_64[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_65 (Convolution1D) (None, 199, 100)      50100       maxpooling1d_26[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_27 (MaxPooling1D)   (None, 39, 100)       0           convolution1d_65[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)              (None, 3900)          0           maxpooling1d_27[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_22 (Dense)                 (None, 2)             7802        flatten_3[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 213,302\n",
      "Trainable params: 213,302\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Merge, Flatten\n",
    "\n",
    "\n",
    "in_sentence = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int64')\n",
    "embedded = Lambda(ohe, output_shape=lambda x: (x[0], x[1], vocab_size), arguments={\"sz\": vocab_size})(in_sentence)\n",
    "\n",
    "conv_block = []\n",
    "filters = [3,4,5]\n",
    "\n",
    "# YOUR CODE\n",
    "\n",
    "model = Model(input=in_sentence, output = result)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>Click to see answer</summary>\n",
    "  <img alt=\"Smiley face\" align=\"left\" src=\"http://1.1m.yt/_FT_6m0.png\">\n",
    "  <p align='left'>Are you sure you tried to solve it on your own?</p>\n",
    "      <pre>\n",
    "            <code>\n",
    "              for i in filters:\n",
    "                  block = Conv1D(100, 5, activation='relu')(embedded)\n",
    "                  conv_block.append(MaxPooling1D(pool_length=3)(block))\n",
    "              merged_convs = Merge(mode='concat', concat_axis=1)(conv_block)\n",
    "              conv_layer = MaxPooling1D(pool_length=5)(Conv1D(100, 5, activation='relu')(merged_convs))\n",
    "              conv_layer = MaxPooling1D(pool_length=5)(Conv1D(100, 5, activation='relu')(conv_layer))\n",
    "              flat_layer = Flatten()(conv_layer)\n",
    "              result = Dense(2, activation='sigmoid')(flat_layer)\n",
    "          </code>\n",
    "      </pre>\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusions:\n",
    "- char models could be very effective for text classification tasks\n",
    "- better results when combinding cnn with lstm\n",
    "- does not require a big vocabulary\n",
    "- can be applied to any language\n",
    "- avoid misspelling errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ConvNets derive their name from the “convolution” operator. The primary purpose of Convolution in case of a ConvNet is to extract features from the input. Convolution preserves the spatial relationship between pixels by learning image features using small squares of input data. We will not go into the mathematical details of Convolution here, but will try to understand how it works over images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a 5 x 5 image whose pixel values are only 0 and 1 (note that for a grayscale image, pixel values range from 0 to 255, the green matrix below is a special case where pixel values are only 0 and 1):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://ujwlkarn.files.wordpress.com/2016/07/screen-shot-2016-07-24-at-11-25-13-pm.png?w=254&h=230)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, consider another 3 x 3 matrix as shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![png](https://ujwlkarn.files.wordpress.com/2016/07/screen-shot-2016-07-24-at-11-25-24-pm.png?w=148&h=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the Convolution of the 5 x 5 image and the 3 x 3 matrix can be computed as shown in the animation in figure below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![animation](https://ujwlkarn.files.wordpress.com/2016/07/convolution_schematic.gif?w=536&h=392)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a moment to understand how the computation above is being done. We slide the orange matrix over our original image (green) by 1 pixel (also called ‘stride’) and for every position, we compute element wise multiplication (between the two matrices) and add the multiplication outputs to get the final integer which forms a single element of the output matrix (pink). Note that the 3×3 matrix “sees” only a part of the input image in each stride.\n",
    "\n",
    "In CNN terminology, the 3×3 matrix is called a ‘filter‘ or ‘kernel’ or ‘feature detector’ and the matrix formed by sliding the filter over the image and computing the dot product is called the ‘Convolved Feature’ or ‘Activation Map’ or the ‘Feature Map‘. It is important to note that filters acts as feature detectors from the original input image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case we have "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
