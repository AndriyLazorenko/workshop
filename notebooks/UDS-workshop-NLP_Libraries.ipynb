{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Data Preprocessing. Fundamentals\n",
    "<h4 style=\"font-size:14px; font-family:Calibry\" align=\"left\"> Natalia Cheilytko </h4>\n",
    "<img width=\"50%\" height=\"50%\" src=\"http://i.piccy.info/i9/666d78be04fbcf04fdb321d5953d1fa5/1492256847/123248/1137898/ua_parrots.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <hr style=\"height: 1px; background-color: #808080\">Table of Contents\n",
    "\n",
    "1. Purpose of Data Preprocessing\n",
    "2. Python Libraries for NLP Tasks: NLTK, TextBlob, Pattern, spaCy\n",
    "3. Preprocessing Features Overview\n",
    "4. Further Reading\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <hr style=\"height: 1px; background-color: #808080\"> 1. Purpose of Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# CRISP DM - Data Science Project Lifecycle\n",
    " <img width=\"35%\" height=\"35%\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/b/b9/CRISP-DM_Process_Diagram.png/800px-CRISP-DM_Process_Diagram.png\">\n",
    "### Textual Data Preparation / Preprocessing\n",
    "The data preparation phase covers all activities to construct the final dataset from the initial raw data, i. e. transformation and cleaning of data for modeling purposes.\n",
    "Textual data requires more tentative preprocessing in order to get it normalized for further modeling, as well as to obtain valuable features from grammatical and semantic peculiarities of text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <hr style=\"height: 1px; background-color: #808080\"> 2. Python Libraries for NLP Tasks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Toolkit\n",
    "http://www.nltk.org/\n",
    "\n",
    "NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextBlob\n",
    "\n",
    "https://textblob.readthedocs.io/en/dev/\n",
    "\n",
    "TextBlob is a Python library for processing textual data. It provides a simple API for diving into common natural language processing (NLP) tasks such as part-of-speech tagging, noun phrase extraction, sentiment analysis, classification, translation, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern\n",
    "\n",
    "http://www.clips.ua.ac.be/pages/pattern-en\n",
    "\n",
    "The pattern.en module contains a fast part-of-speech tagger for English (identifies nouns, adjectives, verbs, etc. in a sentence), sentiment analysis, tools for English verb conjugation and noun singularization & pluralization, and a WordNet interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spaCy\n",
    "\n",
    "https://spacy.io/\n",
    "\n",
    "spaCy is a Python library good for preparing text for deep learning. It interoperates seamlessly with TensorFlow, Keras, Scikit-Learn, Gensim and the rest of Python's awesome AI ecosystem. It's written from the ground up in carefully memory-managed Cython. For text preprocessing, it provides tokenization, syntax-driven sentence segmentation, pre-trained word vectors, part-of-speech tagging, named entity recognition, labelled dependency parsing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See also - Interactive Demo of the NLP Libraries: http://textanalysisonline.com/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height: 1px; background-color: #808080\">\n",
    "## 3. Preprocessing Features Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Let's start with the TextBlob library\n",
    "from textblob import TextBlob\n",
    "from textblob import Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'es'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's detect language of the given sentence\n",
    "text = TextBlob(u\"Beauty and the Beast es una película maravillosa, pero no tan bueno como esperaba.\")\n",
    "text.detect_language()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['Overall', 'my', 'evaluation', 'is', 'now', 'done', 'and', 'I', 'am', 'rooting', 'on', 'the', 'side', 'of', 'it', 'being', 'a', 'brash', 'and', 'exhilarating', 'minor', 'masterpiece', 'Yes', 'it', \"'s\", 'one', 'dimensional', 'Yes', 'it', 'is', 'virtually', 'impossible', 'to', 'feel', 'any', 'empathy', 'with', 'any', 'of', 'the', 'characters', 'as', 'they', 'are', 'all', 'universally', 'loathsome', 'But', 'it', \"'s\", 'a', 'movie', 'whose', 'flaws', 'are', 'forgivable', 'based', 'on', 'the', 'characterisation', 'and', 'the', 'cracking', 'good', 'script', 'by', 'long-term', 'collaborators', 'Ben', 'Wheatley', 'and', 'Amy', 'Jump', 'Tight', 'as', 'it', 'is', 'within', 'its', '90', 'minute', 'running', 'time', 'I', 'doubt', 'you', 'will', 'be', 'bored'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's define tokens for the given extract from a Free Fire (2016) movie review\n",
    "review = TextBlob(u\"Overall, my evaluation is now done and I am rooting on the side of it being a brash and exhilarating minor masterpiece. Yes, it's one- dimensional. Yes, it is virtually impossible to feel any empathy with any of the characters, as they are all universally loathsome. But it's a movie whose flaws are forgivable based on the characterisation and the cracking good script by long-term collaborators Ben Wheatley and Amy Jump. Tight as it is within its 90 minute running time, I doubt you will be bored.\")\n",
    "review.words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sentence(\"Overall, my evaluation is now done and I am rooting on the side of it being a brash and exhilarating minor masterpiece.\"),\n",
       " Sentence(\"Yes, it's one- dimensional.\"),\n",
       " Sentence(\"Yes, it is virtually impossible to feel any empathy with any of the characters, as they are all universally loathsome.\"),\n",
       " Sentence(\"But it's a movie whose flaws are forgivable based on the characterisation and the cracking good script by long-term collaborators Ben Wheatley and Amy Jump.\"),\n",
       " Sentence(\"Tight as it is within its 90 minute running time, I doubt you will be bored.\")]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's split the review to sentences\n",
    "review.sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Normalization: Lemmatization vs. Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/natalia/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/natalia/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/natalia/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/natalia/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package conll2000 to\n",
      "[nltk_data]     /home/natalia/nltk_data...\n",
      "[nltk_data]   Package conll2000 is already up-to-date!\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /home/natalia/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "python3 -m textblob.download_corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization\n",
    "Normalization of any word token to its lemma, i.e. a word base form: children --> child."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['But', 'it', \"'s\", 'a', 'movie', 'whose', 'flaw', 'are', 'forgivable', 'based', 'on', 'the', 'characterisation', 'and', 'the', 'cracking', 'good', 'script', 'by', 'long-term', 'collaborator', 'Ben', 'Wheatley', 'and', 'Amy', 'Jump'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's lemmatize each token in the sentence below\n",
    "sentence = TextBlob(u\"But it's a movie whose flaws are forgivable based on the characterisation and the cracking good script by long-term collaborators Ben Wheatley and Amy Jump.\")\n",
    "sentence.words.lemmatize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming\n",
    "Reducing words to their pseudo (i.e. not necessarily grammatically correct) stem: characterisation --> characteris. \n",
    "There are several stemming algorithms in the NLTK library: Porter Stemmer (default), Snowball Stemmer, Lancaster Stemmer.\n",
    "\n",
    "The overstemming issue should be kept in mind.\n",
    "For example, the widely used Porter Stemmer normalizes \"universal\", \"university\", and \"universe\" to \"univers\". \n",
    "These words are in different domains, so treating them as equals will likely reduce the relevance of many NLP tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['but', 'it', \"'s\", 'a', 'movi', 'whose', 'flaw', 'are', 'forgiv', 'base', 'on', 'the', 'characteris', 'and', 'the', 'crack', 'good', 'script', 'by', 'long-term', 'collabor', 'ben', 'wheatley', 'and', 'ami', 'jump'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's do the stemming for our sentence with the Porter Stemmer.\n",
    "sentence.words.stem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Demo of various lemmatization and stemming algorithms:\n",
    "http://textanalysisonline.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part-of-speech Tagging\n",
    "Default NLTK and Textblob POS tags explained:\n",
    "http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('But', 'CC'),\n",
       " ('it', 'PRP'),\n",
       " (\"'s\", 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('movie', 'NN'),\n",
       " ('whose', 'WP$'),\n",
       " ('flaws', 'NNS'),\n",
       " ('are', 'VBP'),\n",
       " ('forgivable', 'JJ'),\n",
       " ('based', 'VBN'),\n",
       " ('on', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('characterisation', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('the', 'DT'),\n",
       " ('cracking', 'NN'),\n",
       " ('good', 'JJ'),\n",
       " ('script', 'NN'),\n",
       " ('by', 'IN'),\n",
       " ('long-term', 'JJ'),\n",
       " ('collaborators', 'NNS'),\n",
       " ('Ben', 'NNP'),\n",
       " ('Wheatley', 'NNP'),\n",
       " ('and', 'CC'),\n",
       " ('Amy', 'NNP'),\n",
       " ('Jump', 'NNP')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's define POS tags for our sentence with the default NLTK Tagger\n",
    "sentence.pos_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Let's try spaCy for POS tagging and lemmatization, since it is trained to handle social media texts\n",
    "from spacy.en import English\n",
    "parser = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lol NOUN lol\n",
      "that ADJ that\n",
      "is VERB be\n",
      "rly ADV rly\n",
      "funny ADJ funny\n",
      ":) PUNCT :)\n",
      "This DET this\n",
      "is VERB be\n",
      "gr8 VERB gr8\n",
      "i PRON i\n",
      "rate VERB rate\n",
      "it PRON -PRON-\n",
      "8/8 NUM 8/8\n",
      "! PUNCT !\n",
      "! PUNCT !\n",
      "! PUNCT !\n"
     ]
    }
   ],
   "source": [
    "# Let's have a sentence from social media as our example\n",
    "sentence = \"lol that is rly funny :) This is gr8 i rate it 8/8!!!\"\n",
    "parsedSentence = parser(sentence)\n",
    "for token in parsedSentence:\n",
    "    print(token.orth_, token.pos_, token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An Exercise: \n",
    "Compare the spaCy and TextBlob POS tagging for the sentence \"lol that is rly funny :) This is gr8 i rate it 8/8!!!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<details>\n",
    "  <summary>Click to see the answer</summary>\n",
    "       <pre>\n",
    "          <code>\n",
    "            sentence = TextBlob(u\"lol that is rly funny :) This is gr8 i rate it 8/8!!!\")\n",
    "            sentence.pos_tags\n",
    "          </code>\n",
    "      </pre>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopword Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " 'should',\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " 'couldn',\n",
       " 'didn',\n",
       " 'doesn',\n",
       " 'hadn',\n",
       " 'hasn',\n",
       " 'haven',\n",
       " 'isn',\n",
       " 'ma',\n",
       " 'mightn',\n",
       " 'mustn',\n",
       " 'needn',\n",
       " 'shan',\n",
       " 'shouldn',\n",
       " 'wasn',\n",
       " 'weren',\n",
       " 'won',\n",
       " 'wouldn']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's access the NLTK stopword list\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6296296296296297"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's compute what amount of words in our example sentence is not in the stopwords list:\n",
    "def content_fraction(text):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    content = [w for w in text if w.lower() not in stopwords]\n",
    "    return len(content) / len(text)\n",
    "sentence=\"But it's a movie whose flaws are forgivable based on the characterisation and the cracking good script by long-term collaborators Ben Wheatley and Amy Jump.\"\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "content_fraction(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Syntactic Parsing\n",
    "Identification of syntactic structure of a sentence, usually, in terms of either Dependency Grammar or Constituency Grammar.\n",
    " <img src=\"https://upload.wikimedia.org/wikipedia/commons/0/0d/Wearetryingtounderstandthedifference_%282%29.jpg\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tight/JJ/B-ADJP/O as/IN/B-PP/B-PNP it/PRP/B-NP/I-PNP is/VBZ/B-VP/O within/IN/B-PP/B-PNP its/PRP$/B-NP/I-PNP 90/CD/I-NP/I-PNP minute/NN/I-NP/I-PNP running/VBG/B-VP/I-PNP time/NN/B-NP/I-PNP ,/,/O/O I/PRP/B-NP/O doubt/NN/I-NP/O you/PRP/I-NP/O will/MD/B-VP/O be/VB/I-VP/O bored/VBN/I-VP/O ././O/O\n"
     ]
    }
   ],
   "source": [
    "# Let's try the TextBlob Syntactic Parser\n",
    "sentence = TextBlob(\"Tight as it is within its 90 minute running time, I doubt you will be bored.\")\n",
    "print(sentence.parse())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the tags explained: http://www.clips.ua.ac.be/pages/pattern-en#parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### and compare it to the spaCy Parser output via DisplaCy - \n",
    "Dependency Tree Parsing Visualization: https://demos.explosion.ai/displacy/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tight Tight nsubj running\n",
      "it it nsubj is\n",
      "time time dobj running\n",
      "I I nsubj doubt\n",
      "you you nsubjpass bored\n"
     ]
    }
   ],
   "source": [
    "# Let's get noun chunks for our sentence via spaCy:\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "doc = nlp(u'Tight as it is within its 90 minute running time, I doubt you will be bored.')\n",
    "for np in doc.noun_chunks:\n",
    "    print(np.text, np.root.text, np.root.dep_, np.root.head.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Putting all together with spaCy\n",
    "The code below will print for each token in each sentence the following information:\n",
    "- The original form\n",
    "- The lemma\n",
    "- The part-of-speech\n",
    "- The Penn POS (default for NLTK)\n",
    "- The syntactic function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, it's one-dimensional.\n",
      "Yes yes INTJ UH intj\n",
      ", , PUNCT , punct\n",
      "it -PRON- PRON PRP nsubj\n",
      "'s be VERB VBZ ROOT\n",
      "one one NUM CD advmod\n",
      "- - PUNCT HYPH punct\n",
      "dimensional dimensional ADJ JJ acomp\n",
      ". . PUNCT . punct\n",
      "Tight as it is within its 90 minute running time, I doubt you will be bored.\n",
      "Tight tight NOUN NN nsubj\n",
      "as as ADP IN mark\n",
      "it -PRON- PRON PRP nsubj\n",
      "is be VERB VBZ advcl\n",
      "within within ADP IN prep\n",
      "its -PRON- ADJ PRP$ pobj\n",
      "90 90 NUM CD nummod\n",
      "minute minute NOUN NN npadvmod\n",
      "running run VERB VBG ROOT\n",
      "time time NOUN NN dobj\n",
      ", , PUNCT , punct\n",
      "I -PRON- PRON PRP nsubj\n",
      "doubt doubt VERB VBP conj\n",
      "you -PRON- PRON PRP nsubjpass\n",
      "will will VERB MD aux\n",
      "be be VERB VB auxpass\n",
      "bored bore VERB VBN ccomp\n",
      ". . PUNCT . punct\n"
     ]
    }
   ],
   "source": [
    "#import and construct the package\n",
    "from spacy.en import English\n",
    "nlp = English()\n",
    "\n",
    "#Now, let's assume that we work on a simple corpus, constructed from a list of texts.\n",
    "\n",
    "corpus = [\n",
    "    u\"Yes, it's one-dimensional.\",\n",
    "    u\"Tight as it is within its 90 minute running time, I doubt you will be bored.\"]\n",
    "\n",
    "#To parse the corpus all we need to do is \n",
    "docs = [\n",
    "    nlp(d) for d in corpus\n",
    "]\n",
    "for idx,doc in enumerate(docs):\n",
    "    #print \"working on doc {idx}\".format(idx)\n",
    "    for sent in doc.sents:\n",
    "        print(doc)\n",
    "        #for each sentence, print the tokens and their original form,lemma, pos, penn pos tag, and constituent \n",
    "        for token in sent:\n",
    "            print (token.orth_,token.lemma_,token.pos_,token.tag_,token.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition\n",
    "DisplaCy - NER Visualization: \n",
    "https://demos.explosion.ai/displacy-ent/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CARDINAL 70\n",
      "PERSON Geoff Barrow\n",
      "PERSON Ben Salisbury\n",
      "DATE 70\n",
      "ORG Credence Clearwater Revival\n",
      "PERSON John Denver\n",
      "WORK_OF_ART The Real Kids\n"
     ]
    }
   ],
   "source": [
    "#Let's get named entities available in our example\n",
    "review = nlp(u\"A cracking 70' soundtrack, put together by the Portishead duo of Geoff Barrow and Ben Salisbury, involves 70's classics by Credence Clearwater Revival, John Denver and The Real Kids and it's hammered out at top volume over the action. The downside of this effect is that - for my old ears at least - it sometimes make some of the dialogue hard to follow.\")\n",
    "for ent in review.ents:\n",
    "    print(ent.label_, ent.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic Word Relations: WordNet\n",
    "\n",
    "Try it online: http://wordnetweb.princeton.edu/perl/webwn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Let's try WordNet via TextBlob\n",
    "from textblob.wordnet import VERB\n",
    "from textblob.wordnet import NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('funny_story.n.01'),\n",
       " Synset('amusing.s.02'),\n",
       " Synset('curious.s.01'),\n",
       " Synset('fishy.s.02'),\n",
       " Synset('funny.s.04')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, let's see what are the synsets for a given word\n",
    "word = Word(\"funny\")\n",
    "word.synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['an account of an amusing incident (usually with a punch line)',\n",
       " 'arousing or provoking laughter',\n",
       " 'beyond or deviating from the usual or expected',\n",
       " 'not as expected',\n",
       " 'experiencing odd bodily sensations']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Then let's explore meanings of the word\n",
    "word.definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('order.v.01'),\n",
       " Synset('order.v.02'),\n",
       " Synset('order.v.03'),\n",
       " Synset('regulate.v.02'),\n",
       " Synset('order.v.05'),\n",
       " Synset('order.v.06'),\n",
       " Synset('ordain.v.02'),\n",
       " Synset('arrange.v.07'),\n",
       " Synset('rate.v.01')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's narrow the synset search to a certain part of speech\n",
    "word = Word(\"order\")\n",
    "word.get_synsets(pos=VERB)\n",
    "#Try other option: pos=NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('request.v.02')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's get a parent for a given word in a certain meaning \n",
    "word = wordnet.synset('order.v.01')\n",
    "word.hypernyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('call.v.05'),\n",
       " Synset('command.v.02'),\n",
       " Synset('direct.v.01'),\n",
       " Synset('instruct.v.02'),\n",
       " Synset('warn.v.03')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #Let's find out what are the children for a given word in a certain meaning \n",
    "word.hyponyms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment WordNet\n",
    "SentiWordNet by Esuli, Sebastiani - sentiment scores for 145k WordNet synonym sets.\n",
    "http://sentiwordnet.isti.cnr.it/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<breakdown.n.03: PosScore=0.0 NegScore=0.25>\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import sentiwordnet as swn\n",
    "\n",
    "# For a given word in a certain meaning let's find out its polarity assigned\n",
    "word = swn.senti_synset('breakdown.n.03')\n",
    "print(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See the word's negative / positive score\n",
    "word.neg_score()\n",
    "#Try other option: breakdown.pos_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SentiSynset('decelerate.v.01'),\n",
       " SentiSynset('slow.v.02'),\n",
       " SentiSynset('slow.v.03'),\n",
       " SentiSynset('slow.a.01'),\n",
       " SentiSynset('slow.a.02'),\n",
       " SentiSynset('dense.s.04'),\n",
       " SentiSynset('slow.a.04'),\n",
       " SentiSynset('boring.s.01'),\n",
       " SentiSynset('dull.s.08'),\n",
       " SentiSynset('slowly.r.01'),\n",
       " SentiSynset('behind.r.03')]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Find the nearest sentiment words for a word given\n",
    "list(swn.senti_synsets('slow'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Further Reading\n",
    "\n",
    "1. Natural Language Processing with Python – Analyzing Text with the Natural Language Toolkit. Steven Bird, Ewan Klein, and Edward Loper (http://www.nltk.org/book/)\n",
    "2. TextBlob: Simplified Text Processing (https://textblob.readthedocs.io/en/dev/index.html)\n",
    "3. Spacy Documentation (https://spacy.io/docs/usage/)\n",
    "4. Intro to NLP with spaCy (https://nicschrading.com/project/Intro-to-NLP-with-spaCy/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
